{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91ccb382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import clone\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b0c23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILES = [\"project1_dataset1.txt\", \"project1_dataset2.txt\"]  # update paths if needed\n",
    "SAVE_RESULTS = True\n",
    "OUT_DIR = \"classification_outputs\"\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6a3d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models definitions (you can tune these hyperparams)\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, random_state=RANDOM_STATE),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, probability=True, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "    # AdaBoost will be created per fold with DecisionTree base\n",
    "    \"NeuralNet\": MLPClassifier(hidden_layer_sizes=(100, ), activation=\"relu\",\n",
    "                               solver=\"adam\", alpha=1e-4, max_iter=400, random_state=RANDOM_STATE)\n",
    "}\n",
    "# AdaBoost base\n",
    "adaboost_base = DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE)\n",
    "adaboost = AdaBoostClassifier(base_estimator=adaboost_base, n_estimators=50, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac9a727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric functions\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1dea639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠️ File not found: {path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Try both tab and comma separators automatically\n",
    "        df = pd.read_csv(path, sep=None, engine=\"python\")\n",
    "        print(f\"Loaded: {path} with shape {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c237520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframes(df):\n",
    "    \"\"\"\n",
    "    df: pandas DataFrame loaded without header. Assumes last column is label.\n",
    "    Returns: X DataFrame (original column names preserved as ints), y Series\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # Drop rows with all NaN\n",
    "    df.dropna(how=\"all\", inplace=True)\n",
    "    # Last column is label\n",
    "    y = df.iloc[:, -1].astype(int)\n",
    "    X = df.iloc[:, :-1]\n",
    "    # Detect columns that are numeric vs object/nominal\n",
    "    # Some numeric columns may be parsed as object if they contain strings; we'll try coercion\n",
    "    numeric_mask = []\n",
    "    for col in X.columns:\n",
    "        # attempt to coerce to numeric\n",
    "        coerced = pd.to_numeric(X[col], errors=\"coerce\")\n",
    "        non_null_fraction = coerced.notna().mean()\n",
    "        # if most entries can be coerced to numeric, treat as numeric\n",
    "        if non_null_fraction > 0.9:\n",
    "            X[col] = coerced\n",
    "            numeric_mask.append(True)\n",
    "        else:\n",
    "            # leave as object/categorical\n",
    "            X[col] = X[col].astype(str)\n",
    "            numeric_mask.append(False)\n",
    "    numeric_mask = np.array(numeric_mask)\n",
    "    return X, y, numeric_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1441d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocessor(X, numeric_mask):\n",
    "    \"\"\"\n",
    "    Builds a ColumnTransformer that imputes & scales numeric columns and imputes & one-hot encodes categorical columns.\n",
    "    \"\"\"\n",
    "    numeric_cols = [c for c, m in zip(X.columns, numeric_mask) if m]\n",
    "    cat_cols = [c for c, m in zip(X.columns, numeric_mask) if not m]\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\", fill_value=\"missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", cat_transformer, cat_cols)\n",
    "    ], remainder=\"drop\")\n",
    "    return preprocessor, numeric_cols, cat_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "250b0974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing file: project1_dataset1.txt ---\n",
      "Loaded: project1_dataset1.txt with shape (568, 31)\n",
      "Loaded shape: (568, 31)\n",
      "Detected 30 numeric columns and 0 nominal columns.\n",
      "Saved summary and per-fold predictions to classification_outputs/\n",
      "\n",
      "Summary (means ± std):\n",
      "SVM          | Acc 0.9736 ± 0.0212 | Prec 0.9643 ± 0.0422 | Rec 0.9671 ± 0.0296 | F1 0.9651 ± 0.0272\n",
      "NeuralNet    | Acc 0.9736 ± 0.0117 | Prec 0.9772 ± 0.0300 | Rec 0.9526 ± 0.0301 | F1 0.9641 ± 0.0161\n",
      "KNN          | Acc 0.9684 ± 0.0172 | Prec 0.9855 ± 0.0222 | Rec 0.9290 ± 0.0381 | F1 0.9559 ± 0.0240\n",
      "AdaBoost     | Acc 0.9560 ± 0.0252 | Prec 0.9569 ± 0.0328 | Rec 0.9242 ± 0.0529 | F1 0.9394 ± 0.0350\n",
      "NaiveBayes   | Acc 0.9279 ± 0.0362 | Prec 0.9156 ± 0.0774 | Rec 0.8959 ± 0.0508 | F1 0.9032 ± 0.0456\n",
      "DecisionTree | Acc 0.9245 ± 0.0359 | Prec 0.8988 ± 0.0592 | Rec 0.9004 ± 0.0582 | F1 0.8984 ± 0.0486\n",
      "\n",
      "--- Processing file: project1_dataset2.txt ---\n",
      "Loaded: project1_dataset2.txt with shape (461, 10)\n",
      "Loaded shape: (461, 10)\n",
      "Detected 8 numeric columns and 1 nominal columns.\n",
      "Saved summary and per-fold predictions to classification_outputs/\n",
      "\n",
      "Summary (means ± std):\n",
      "NaiveBayes   | Acc 0.7027 ± 0.0672 | Prec 0.5673 ± 0.0828 | Rec 0.6375 ± 0.1179 | F1 0.5965 ± 0.0895\n",
      "SVM          | Acc 0.6854 ± 0.0481 | Prec 0.5396 ± 0.0546 | Rec 0.6687 ± 0.0687 | F1 0.5963 ± 0.0563\n",
      "NeuralNet    | Acc 0.7068 ± 0.0838 | Prec 0.5980 ± 0.1495 | Rec 0.5062 ± 0.1324 | F1 0.5434 ± 0.1323\n",
      "AdaBoost     | Acc 0.6789 ± 0.0695 | Prec 0.5447 ± 0.1133 | Rec 0.4813 ± 0.1371 | F1 0.5047 ± 0.1170\n",
      "DecisionTree | Acc 0.6377 ± 0.0401 | Prec 0.4773 ± 0.0566 | Rec 0.4813 ± 0.0970 | F1 0.4765 ± 0.0713\n",
      "KNN          | Acc 0.6702 ± 0.0552 | Prec 0.5314 ± 0.0941 | Rec 0.4125 ± 0.1256 | F1 0.4576 ± 0.1113\n",
      "\n",
      "Combined summary saved to classification_outputs/combined_results_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- Main experiment function ----------\n",
    "def run_experiments(path):\n",
    "    print(f\"\\n--- Processing file: {path} ---\")\n",
    "    df = load_dataset(path)\n",
    "    print(f\"Loaded shape: {df.shape}\")\n",
    "    X_raw, y, numeric_mask = prepare_dataframes(df)\n",
    "    print(f\"Detected {numeric_mask.sum()} numeric columns and {len(numeric_mask)-numeric_mask.sum()} nominal columns.\")\n",
    "\n",
    "    preprocessor, numeric_cols, cat_cols = make_preprocessor(X_raw, numeric_mask)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Prepare structures to store per-model fold metrics\n",
    "    results = {name: [] for name in list(models.keys()) + [\"AdaBoost\"]}\n",
    "    fold_idx = 0\n",
    "\n",
    "    # optionally store per-fold predictions\n",
    "    per_fold_records = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X_raw, y):\n",
    "        fold_idx += 1\n",
    "        X_train_raw = X_raw.iloc[train_idx]\n",
    "        X_test_raw = X_raw.iloc[test_idx]\n",
    "        y_train = y.iloc[train_idx].values\n",
    "        y_test = y.iloc[test_idx].values\n",
    "\n",
    "        # Fit preprocessor on training data and transform both\n",
    "        preprocessor.fit(X_train_raw)\n",
    "        X_train = preprocessor.transform(X_train_raw)\n",
    "        X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "        # For each model, fit and evaluate\n",
    "        for name, clf in models.items():\n",
    "            model = clone(clf)\n",
    "            # For tree and NB, scaling is okay but not required. For KNN & SVM scaling matters; we've already scaled numerics.\n",
    "            # Fit\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            met = compute_metrics(y_test, y_pred)\n",
    "            results[name].append(met)\n",
    "            # optionally save per-fold preds\n",
    "            for i_true, i_pred, idx in zip(y_test, y_pred, test_idx):\n",
    "                per_fold_records.append({\n",
    "                    \"file\": os.path.basename(path),\n",
    "                    \"fold\": fold_idx,\n",
    "                    \"index\": int(idx),\n",
    "                    \"method\": name,\n",
    "                    \"y_true\": int(i_true),\n",
    "                    \"y_pred\": int(i_pred)\n",
    "                })\n",
    "\n",
    "        # AdaBoost using DecisionTree base\n",
    "        ada = clone(adaboost)\n",
    "        ada.fit(X_train, y_train)\n",
    "        y_pred = ada.predict(X_test)\n",
    "        met = compute_metrics(y_test, y_pred)\n",
    "        results[\"AdaBoost\"].append(met)\n",
    "        for i_true, i_pred, idx in zip(y_test, y_pred, test_idx):\n",
    "            per_fold_records.append({\n",
    "                \"file\": os.path.basename(path),\n",
    "                \"fold\": fold_idx,\n",
    "                \"index\": int(idx),\n",
    "                \"method\": \"AdaBoost\",\n",
    "                \"y_true\": int(i_true),\n",
    "                \"y_pred\": int(i_pred)\n",
    "            })\n",
    "\n",
    "    # Summarize: compute mean ± std for each metric\n",
    "    summary_rows = []\n",
    "    for name, mets in results.items():\n",
    "        # mets is list of dicts (one per fold)\n",
    "        accs = [m[\"accuracy\"] for m in mets]\n",
    "        precs = [m[\"precision\"] for m in mets]\n",
    "        recs = [m[\"recall\"] for m in mets]\n",
    "        f1s = [m[\"f1\"] for m in mets]\n",
    "        summary_rows.append({\n",
    "            \"file\": os.path.basename(path),\n",
    "            \"method\": name,\n",
    "            \"accuracy_mean\": np.mean(accs),\n",
    "            \"accuracy_std\": np.std(accs),\n",
    "            \"precision_mean\": np.mean(precs),\n",
    "            \"precision_std\": np.std(precs),\n",
    "            \"recall_mean\": np.mean(recs),\n",
    "            \"recall_std\": np.std(recs),\n",
    "            \"f1_mean\": np.mean(f1s),\n",
    "            \"f1_std\": np.std(f1s)\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values([\"file\", \"f1_mean\"], ascending=[True, False])\n",
    "    per_fold_df = pd.DataFrame(per_fold_records)\n",
    "\n",
    "    if SAVE_RESULTS:\n",
    "        os.makedirs(OUT_DIR, exist_ok=True)\n",
    "        summary_df.to_csv(os.path.join(OUT_DIR, f\"results_summary_{os.path.basename(path)}.csv\"), index=False)\n",
    "        per_fold_df.to_csv(os.path.join(OUT_DIR, f\"per_fold_predictions_{os.path.basename(path)}.csv\"), index=False)\n",
    "        print(f\"Saved summary and per-fold predictions to {OUT_DIR}/\")\n",
    "\n",
    "    # Print nicely\n",
    "    pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "    print(\"\\nSummary (means ± std):\")\n",
    "    for _, row in summary_df.iterrows():\n",
    "        print(f\"{row['method']:12s} | Acc {row['accuracy_mean']:.4f} ± {row['accuracy_std']:.4f} | \"\n",
    "              f\"Prec {row['precision_mean']:.4f} ± {row['precision_std']:.4f} | \"\n",
    "              f\"Rec {row['recall_mean']:.4f} ± {row['recall_std']:.4f} | \"\n",
    "              f\"F1 {row['f1_mean']:.4f} ± {row['f1_std']:.4f}\")\n",
    "    return summary_df, per_fold_df\n",
    "\n",
    "# Run on each file\n",
    "all_summaries = []\n",
    "all_per_fold = []\n",
    "for fpath in DATA_FILES:\n",
    "    if not os.path.exists(fpath):\n",
    "        print(f\"Warning: file not found: {fpath}. Skipping. (Please place it in the working dir or update DATA_FILES.)\")\n",
    "        continue\n",
    "    s, p = run_experiments(fpath)\n",
    "    all_summaries.append(s)\n",
    "    all_per_fold.append(p)\n",
    "\n",
    "# Combine and save overall summary\n",
    "if SAVE_RESULTS and all_summaries:\n",
    "    combined = pd.concat(all_summaries, ignore_index=True)\n",
    "    combined.to_csv(os.path.join(OUT_DIR, \"combined_results_summary.csv\"), index=False)\n",
    "    print(f\"\\nCombined summary saved to {OUT_DIR}/combined_results_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
